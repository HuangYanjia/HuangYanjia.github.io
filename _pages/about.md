---
layout: about
title: about
permalink: /
subtitle: >
  <a href="https://engineering.tamu.edu/entc/index.html" target="_blank">Texas A&amp;M University</a> &nbsp;·&nbsp;
  Graduate Research Assistant &nbsp;·&nbsp;
  Robotics &amp; Embodied AI

profile:
  align: right
  image: image.png
  image_circular: false # crops the image to make it circular
  more_info: >
    <p>Mechanical Engineering</p>
    <p>Texas A&amp;M University</p>
    <p>College Station, TX 77840, USA</p>
    <p><a href="mailto:yanjia_0812@tamu.edu">yanjia_0812@tamu.edu</a></p>

news: false  # includes a list of news items 
<!-- latest_posts: false  # includes a list of the newest posts -->
selected_papers: false # includes a list of papers marked as "selected={true}" 
social: false  # includes social icons at the bottom of the page
announcements:
  enabled: false
  scrollable: true
  limit: 5

latest_posts:
  enabled: false
  scrollable: true
  limit: 3
---


I am **Yanjia Huang**, an M.S. student in Mechanical Engineering at **Texas A&M University**, where I am a member of the <a href="https://taco-group.github.io/" target="_blank">TACO Group</a> led by Prof. <a href="https://vztu.github.io/" target="_blank">Zhengzhong Tu</a>.
My research explores how *vision-language models*, *diffusion policy learning* can be woven together to produce robust, long-horizon plans for **embodied agents**—from indoor navigation to dexterous manipulation.

Prior to my graduate studies at Texas A&M, I completed a dual-degree program in Mechanical Engineering at the <a href="http://www.sbcen.usst.edu.cn/" target="_blank">Sino-British College (SBC), USST</a> and <a href="https://www.ljmu.ac.uk/" target="_blank">Liverpool John Moores University (LJMU)</a>, where I also pursued a minor in Physics. My passion for research was ignited during my time as a research assistant with Prof. <a href="http://yanweifu.github.io/" target="_blank">Yanwei Fu</a> at <a href="https://www.fudan.edu.cn/en/" target="_blank">Fudan University</a> and at NYU's <a href="https://ai4ce.github.io/" target="_blank">AI4CE Lab</a> (advised by Prof. <a href="https://engineering.nyu.edu/faculty/chen-feng" target="_blank">Chen Feng</a>) and <a href="https://yifang.org/group.html" target="_blank">Multimedia and Visual Computing Lab (MMVC)</a> (advised by Prof. <a href="https://nyuad.nyu.edu/en/academics/divisions/engineering/faculty/yi-fang.html" target="_blank">Yi Fang</a>). This passion was further solidified during my internship at <a href="https://www.huawei.com/en/corporate-information/research-development" target="_blank">Huawei's Noah's Ark Lab</a>, mentored by <a href="https://xuhangcn.github.io/" target="_blank">Hang Xu</a>.

Get full <a href="/assets/pdf/CV_Yanjia_Huang.pdf" target="_blank">curriculum vitae</a> here.

Have a collaboration idea or just want to chat? Shoot me an email—coffee’s on me.


<!-- Outside the lab you’ll find me at a piano, on a hiking trail, or cycling. -->
> *“Robots shouldn’t just imitate; they should **imagine**, **reason**, and **plan**.”*

<!-- **Recent highlights**

* **VISTA** — Developed VISTA, a novel scheduling framework that leverages a diffusion model for "visual imagination," enabling embodied agents to proactively plan and recover from low-confidence states.
* **PANDORA** — Designed PANDORA, a diffusion-based control policy that generates fine-grained, expressive motor commands for the complex task of robotic piano playing. (IROS 2025, under review)
* Applied **Monte-Carlo Tree Diffusion (MCTD)** to enhance planning in Vision-Language Agents (VLAs), enabling efficient search over long-horizon, goal-conditioned motions for zero-shot object manipulation.   -->




### Featured Publications

<div style="margin-bottom: 2.5rem;">
  <div class="row">
    <div class="col-sm-5">
      <img src="/assets/img/publication_preview/PANDORA.jpg" class="img-fluid rounded z-depth-1" alt="PANDORA preview"/>
    </div>
    <div class="col-sm-7">
      <h5><a href="https://taco-group.github.io/PANDORA/" target="_blank" class="font-weight-bold">PANDORA: Diffusion Policy Learning for Dexterous Robotic Piano Playing</a></h5>
      <p>Yanjia Huang, Renjie Li, & Zhengzhong Tu</p>
      <p>arXiv preprint, 2025</p>
      <p>
        <a href="https://arxiv.org/abs/2503.14545" class="btn btn-sm btn-outline-primary" role="button" target="_blank">PDF</a>
        <a href="https://taco-group.github.io/PANDORA/" class="btn btn-sm btn-outline-primary" role="button" target="_blank">Website</a>
      </p>
    </div>
  </div>
</div>
<div style="margin-bottom: 2.5rem;">
  <div class="row">
    <div class="col-sm-5">
      <img src="/assets/img/publication_preview/VISTA.png" class="img-fluid rounded z-depth-1" alt="VISTA preview"/>
    </div>
    <div class="col-sm-7">
      <h5><a href="https://arxiv.org/abs/2505.07868" target="_blank" class="font-weight-bold">VISTA: Generative Visual Imagination for Vision-and-Language Navigation</a></h5>
      <p>Yanjia Huang, Mingyang Wu, Renjie Li, & Zhengzhong Tu</p>
      <p>arXiv preprint, 2025</p>
      <p>
        <a href="https://arxiv.org/abs/2505.07868" class="btn btn-sm btn-outline-primary" role="button" target="_blank">PDF</a>
      </p>
    </div>
  </div>
</div>
<div style="margin-bottom: 2.5rem;">
  <div class="row">
    <div class="col-sm-5">
      <img src="/assets/img/publication_preview/softrobot.jpg" class="img-fluid rounded z-depth-1" alt="Self-healing Actuator preview"/>
    </div>
    <div class="col-sm-7">
      <h5><a href="https://www.sciencedirect.com/science/article/pii/S1359836821001402" target="_blank" class="font-weight-bold">A self-healing composite actuator for multifunctional soft robot via photo-welding</a></h5>
      <p>Mingxia Liu, Shu Zhu, Yanjia Huang, Zihui Lin, Weiping Liu, Lili Yang, & Dengteng Ge</p>
      <p>Composites Part B: Engineering, 2021</p>
      <p>
        <a href="https://doi.org/10.1016/j.compositesb.2021.108748" class="btn btn-sm btn-outline-primary" role="button" target="_blank">DOI</a>
      </p>
    </div>
  </div>
</div>
<div style="margin-bottom: 2.5rem;">
  <div class="row">
    <div class="col-sm-5">
      <img src="/assets/img/publication_preview/L-ZSON.png" class="img-fluid rounded z-depth-1" alt="L-ZSON preview"/>
    </div>
    <div class="col-sm-7">
      <h5><a href="https://vlt-lzson.github.io/" target="_blank" class="font-weight-bold">Zero-shot Object Navigation with Vision-Language Models Reasoning</a></h5>
      <p>Congcong Wen, Yisiyuan Huang, Hao Huang, Yanjia Huang, Shuaihang Yuan, Yu Hao, Hui Lin, Yu-Shen Liu, & Yi Fang</p>
      <p>ICPR, 2024</p>
      <p>
        <a href="https://arxiv.org/abs/2410.18570" class="btn btn-sm btn-outline-primary" role="button" target="_blank">PDF</a>
        <a href="https://vlt-lzson.github.io/" class="btn btn-sm btn-outline-primary" role="button" target="_blank">Website</a>
      </p>
    </div>
  </div>
</div>
<div style="margin-bottom: 2.5rem;">
  <div class="row">
    <div class="col-sm-5">
      <img src="/assets/img/publication_preview/MapBench.png" class="img-fluid rounded z-depth-1" alt="MapBench preview"/>
    </div>
    <div class="col-sm-7">
      <h5><a href="https://arxiv.org/abs/2503.14607" target="_blank" class="font-weight-bold">Can Large Vision Language Models Read Maps Like a Human?</a></h5>
      <p>Shuo Xing, Zezhou Sun, Shuangyu Xie, Kaiyuan Chen, Yanjia Huang, Yuping Wang, Jiachen Li, Dezhen Song, & Zhengzhong Tu</p>
      <p>arXiv preprint, 2025</p>
      <p>
        <a href="https://arxiv.org/abs/2503.14607" class="btn btn-sm btn-outline-primary" role="button" target="_blank">PDF</a>
      </p>
    </div>
  </div>
</div>